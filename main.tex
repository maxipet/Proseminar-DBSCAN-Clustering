%default language is German. use the second line instead for english settings:
\RequirePackage{ifpdf}
\documentclass{lni}
%\documentclass[english]{lni}

%\IfFileExists{latin1.sty}{\usepackage{latin1}}{\usepackage{isolatin1}}

\usepackage{graphicx}
\usepackage{svg}
\usepackage{adjustbox}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}

\lstset{
    language=Java,
    basicstyle=\fontsize{10}{13}\selectfont\ttfamily, 
    morekeywords={bool},                % added Keyword "bool"
    keywordstyle=\bfseries,             % keywords fat
    commentstyle=\color{gray},          % grey comments
    showstringspaces=false
}  

\renewcommand{\labelenumi}{\theenumi)}

\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[-] #1}
}

\author{
	Maximilian Petri \\ 
	\\ 
	Informatik 11 -- Embedded Software \\ 
	RWTH Aachen University \\ 
	Aachen, Germany \\ 
	maximilian.petri@rwth-aachen.de\\
	\\
	\textit{Betreuer}\\
	\textit{Alexander Kugler}\\ 
}
\title{\small{Proseminar} \\ \vspace{0.5cm} \Large{DBSCAN Clustering}}

\begin{document}
\maketitle

%
% Abstract
%

\begin{abstract}
Im Bereich Data Mining existieren zahlreiche Methoden, um aus gesammelten Daten Muster und somit auch neues Wissen zu extrahieren. Normalerweise liegen diese Muster in Form von Clustern, also Anhäufungen von Datensätzen, vor. Es gibt eine Vielzahl von Clustering Algorithmen, welche genau diese Aufgabe erledigen und somit ohne menschliche Hilfe aus Daten neues Wissen schaffen können. In dieser Arbeit wird der 1996 vorgestellen DBSCAN Algorithmus thematisch noch einmal aufbearbeitet und mit anderen Clustering Verfahren verglichen. Es wird außerdem bewertet ob DBSCAN die enormen Herausforderungen, welche in Folge von Big Data entstehen, bewältigen kann.
\end{abstract}

%
% Einführung
%

\section{Einführung}
Die Cluster Analyse gehört zu den Knowledge-Discovery Methoden und versteht sich als Teildisziplin des Data Minigs. Sie hat zahlreiche Anwendungen in Wissenschaft und Wirtschaft, weshalb zuverlässige Algorithmen benötigt werden, sodass diese Aufgaben automatisiert erledigt werden können. Ein Anwendungsbeispiel wäre die Analyse des Kaufverhaltens von Kunden auf einer Website, um mögliche Zielgruppen und ihre Interressen einfacher zu gruppieren. Es lässt sich leicht vorstellen, dass die Datenmenge in solchen Anwendungen sehr groß werden kann. Deshalb ist die Effizienz dieser Algorithmen nicht zu vernachlässigen.\\
In diesem Proseminar wird der DBSCAN Algorithmus aufbearbeitet. Hierfür werden zuerst in \hyperref[sec:grund]{Abschnitt 2} die Grundlagen für DBSCAN vorgestellt, welche benötigt werden um den Algorithmus, welcher in \hyperref[sec:kern]{Abschnitt 4} vorgestellt wird, zu verstehen.
In \hyperref[sec:perf]{Abschnitt 5} soll dann DBSCAN mit den clustering Verfahren SLINK, $k$-Means und OPTICS verglichen werden. Hierbei wird besonders auf die Anforderungen geachtet, welche durch Big Data entstehen. Anschließend wird in \hyperref[sec:fazit]{Abschnitt 6} beurteilt, ob die Verwendung DBSCAN noch zeitgemäß ist und welche Alternativen zur Verfügung stehen.

%
% Grundlagen
%

\section{Grundlagen}
\label{sec:grund}
Im folgenden Abschnitt werden wichtige mathematische Grundlagen für das DBSCAN Verfahren erläutert und auf die Datenstruktur R*-Tree eingegangen, welche eine effiziente Implementierung von DBSCAN ermöglicht.

% Daten

\subsection{Daten}
Daten sind die Objekte in denen DBSCAN Cluster sucht. Die in dieser Arbeit betrachteten Datenpunkte sind alle in der Form $p = (p_1, \dots, p_n) \in \mathbb{R}^n$.
Unsere Datenmenge $D \subseteq \mathbb{R}^n$ ist dann die Menge, welche alle Datenpunkte, die betrachtet werden, beinhaltet. Somit haben unsere Datenpunkte $n$ verschiedene Datenfelder. In den Abbildungen und Beispielen dieser Arbeit wird sich auf $n=2$ beschränkt, da dies am verständlichsten zu visualisieren ist.

% Abstand

\subsection{Abstand}
Der Ansatz von DBSCAN zur Erkennung von Clustern benötigt die Berechnung von Abständen zwischen Datenpunkten, da diese die Grundlage der im nächsten Abschnitt definierten Umgebungen sind. Es gibt eine Vielzahl von verschiedenen Metriken, welche verwendet werden können um den Abstand zu definieren. Im weiteren Verlauf dieser Arbeit wird jedoch der Euklidische Abstand betrachtet, da dieser dem intuitiven Abstand in z.B. einer 2-Dimensionalen Ebene entspricht und deshalb gut für Demonstrationen bzw. Abbildungen geeignet ist. Der Abstand zwischen 2 Punkten $p,q \in D$ ist definiert als
\begin{align}
    d(p,q) = \sqrt{\sum_{i=1}^{n}{(p_i+q_i)^2}} = \sqrt{(p_1+q_1)^2 + \dots + (p_n+q_n)^2} \notag
\end{align}
Bei der eigentlichen Implementation von DBSCAN kann jedoch eine beliebige Metrik gewählt werden, welche jeweils andere Ergebnisse erziehlen kann.
Die Wahl der richtigen Metrik soll hier aber nicht weiter diskutiert werden.

% Umgebung

\subsection{Umgebung}
Eine Umgebung um einen Punkt ist die natürliche Erweiterung der zuletzt definierten Abstandsfunktion. Hiermit ist die Menge aller Punkte gemeint, welche innerhalb eines gegebenen Abstands zu einem bestimmten Punkt liegen.
Definiert wird die Umgebung von Punkt $p \in D$ bezüglich des max. Abstands $eps \in \mathbb{R}_{>0}$ als
\begin{align}
    U_{eps}(p) = \{ q \in D \enspace | \enspace d(p,q) < r \} \notag
\end{align}
Die grundlegende Methode von DBSCAN zur Bestimmung von Clustern beruht auf der Berechnung der Umgebungen von bestimmte Punkten, weshalb dieses Konzept sehr wichtig für den Algorithmus ist und somit effizient implementiert werden muss.

% R*-Tree

\subsection{R*-Tree}
Um das Ermitteln der Umgebung eines Punktes effizient zu gestalten, wird auf die Datenstruktur R*-Tree \cite{R*} zurückgegriffen. Diese ermöglicht es die Umgebung eines Punktes in $O(\log n)$ zu finden, wobei hier $n$ die Anzahl der Datenpunkte ist, also $|D| = n$. Würde man beispielsweise eine Liste verwenden um die Datenmenge zu speichern, bräuchte man $O(n)$ um die Umgebung zu bestimmen. Deshalb ist die Verwendung von R*-Trees unerlässlich um eine effiziente Implementierung des Algorithmus zu gewährleisten.

%
% Verwandte Arbeiten
%

\section{Verwandte Arbeiten}
Nachdem nun die Grundlagen für den Algorithmus erklärt wurden, richtet sich der nächste Abschnitt an wichtige Veröffentlichungen für diese Arbeit. Es werden unteranderem 3 weitere Clustering Algorithmen vorgestellt, mit welchen DBSCAN in \hyperref[sec:perf]{Abschnitt \ref{sec:perf}} verglichen wird. Zu beginn sollte jedoch auf die Veröffentlichung von DBSCAN selbst eingangen werden.

% Original Paper

\subsection{Original Paper}
Der DBSCAN Algorithmus wurde von Kriegel et al. 1996 vorgestellt \cite{DBSCAN} und ist einer der am häufigsten benutzten Algorithmen für die Clusteranalyse. Er verwendete als erster das Konzept des Dichte basierten Clusterings. Das Paper gehört außerdem zu den am meisten zitierten Publikationen im Bereich Data Mining und gewann im Jahr 2014 den \glqq SIGKDD Test of Time award \grqq{} \cite{TOTA}.

% OPTICS

\subsection{OPTICS}
OPTICS wurde 1999 als eine Weiterentwicklung von DBSCAN veröffentlicht \cite{OPTICS}. Das Verfahren verwendet das selbe, auf der Dichte von Datenpunkten beruhende, Konzept zur Identifikation von Clustern. Jedoch ist OPTICS so gestalltet das es Cluster verschiedener Dichten erkennen kann und so unter anderem im Vergleich zu DBSCAN keinen $\epsilon$ Parameter benötigt. Somit muss bei der Verwendung von ein Wert weniger angepasst werden, was zu einer einfacheren Benutzung führt. OPTICS greift genau wie DBSCAN auf die R*-Tree Datenstruktur zurück, mitwelcher Laufzeiten von $O(n\log n)$ bezüglich der Datenmenge erreicht werden.

% K-Means

\subsection{$k$-Means}
Es soll nun auf einen Vertreter der Methode des \textit{partitionierenden} Clusterings näher eingegangen werden. $k$-Means \cite{TOP10} ist ein sehr einfacher, aber effektiver Algortihmus zur Aufteilung bzw. Partitionierung der Datenmenge. Das Ziel ist es jedes Cluster in einem eigenen Teil dieser Partitionierung zu haben. $k$-Means passt hierfür iterativ $k$ verschiedene Punkte so an, dass eine bestimmte Zielfunktion minimiert wird, welche so definiert ist, dass wenn man ein lokales Minimum erreicht, die Partitionierung eine gute Approximation für die Cluster ergeben soll. Hierbei ergibt sich jedoch das Problem, dass man immer nur $k$ verschiedene Cluster bestimmen kann, weshalb dieser Parameter an die Datenmenge angepasst werden muss.

% SLINK

\subsection{SLINK}
Als ein Algorithmus, welcher auf dem Prinzip des hierarchischen Clusterings, wird an dieser Stelle Single-Linkage-Clustering  (SLINK)\cite{HIER} vorgestellt. SLINK versucht in mehreren Schritten aus der Menge aller Punkte einen Baum zu erstellen, bei welchem dann die einzelnen Cluster zusammen hängen. Der Algorithmus kombiniert hierfür immer die beiden \textit{nächstgelegenen} Punkte, welche noch nicht kombiniert wurden. Hierbei bedeutet \textit{nächstgelegen}, dass immer die beiden Punkte gewählt werden, welche minimalen Abstand voneinander haben. Da der Algorithmus aus den einzelnen Punkten die Menge der Cluster aufbaut, gehört er zur Gruppe der agglomerativen Verfahren des hierarchischen Clusterings.

%
% DBSCAN
%

\section{DBSCAN}
In diesem Abschnitt soll der DBSCAN Algorithmus vorgestellt werden.
Hierfür werden zuerst die Kernkonzepte in \hyperref[sec:kern]{Abschnitt 4.1} erklärt, welche die Grundlage für das Verfahren bilden. Im darauf folgenden \hyperref[sec:impl]{Abschnitt 4.2} wird dann die Implementation von DBSCAN beschrieben und auf weitere Ideen zur Verwendung des Verfahrens eingegangen.

%
% Kernkonzepte
%

\subsection{Kernkonzepte}
\label{sec:kern}
DBSCAN benötigt 2 Parameter zur Bestimmung von Clustern, nämlich $eps \in \mathbb{R}$ und $minPoints \in \mathbb{N}$. Hierbei legt $eps$ die Größe der Umgebungen um die Punkte in der Datenmenge fest und $minPoints$ wird benötigt zur Definition eines Kernpunktes.\\

% Kernpunkt

\textbf{Kernpunkt}\\
Das erste wichtige Konzept für DBSCAN ist die Definition eines Kernpunktes. Hiermit sind die Punkte in der Datenmenge gemeint, welche innerhalb ihres näheren Umfelds, genauer innerhalb ihrer $eps$-Umgebung, mindestens eine Anzahl von $minPoints$ Punkten haben. Definiert ist ein Kernpunkt dann wie folgt:\\
Ein Punkt $ p \in D $ ist \textit{Kernpunkt}, falls
\begin{itemize}
    \item $ | U_{eps}(p) | \geq minPoints  $
\end{itemize}
Sei $eps = 1$ und $minPoints = 5$, dann ist Punkt A in Abbildung \ref{fig:1}a) ein Kernpunkt, da innerhalb seiner Umgebung (dargestellt durch den gestrichelten Kreis) mindestens 5 Punkte liegen (dabei wird der Punkt A selbst mitgezählt). Punkt B hingegen ist kein Kernpunkt, denn in seiner Umgebung liegen nur 2 Punkte.\\

% Direkte Dichte-Erreichbarkeit

\textbf{Direkte Dichte-Erreichbarkeit}\\
Das nächste wichtige Konzept, ist das der \textit{direkten Dichte-Erreichbarkeit}. Diese nicht symmetrische Relation setzt immer einen Kernpunkt in Relation zu allen anderen Punkten in seiner Umgebung. Das bedeutet das alle Punkte innerhalb der Umgebung eines Kernpunktes \textit{direkt dichte-erreichbar} von diesem sind. Somit ergibt sich folgende Definition:\\
Ein Punkt $ p \in D $ ist \textit{direkt dichte-erreichbar} von k, falls
\begin{itemize}
    \item $k$ ist \textit{Kernpunkt}
    \item $ p \in U_{eps}(k) $
\end{itemize}
In Abbildung \ref{fig:1}b) wäre (mit den gleichen Parametern $eps$ und $minPoints$ wie zuvor) dann A \textit{direkt dichte-erreichbar} von Kernpunkt K, weil K $\in U_{eps}(\text{A})$. Da K nicht in $U_{eps}(\text{B})$ liegt, folgt das B nicht \textit{direkt dichte-erreichbar} von K ist.

% Abbildung 1

\begin{figure}[hb]
    \centering
    \subfloat[]{{\includesvg[width=0.45\textwidth]{figures/figure1.svg}}}%
    \qquad
    \subfloat[]{{\includesvg[width=0.45\textwidth]{figures/figure2.svg}}}
    \caption{Kernpunke und direkt Dichte-Erreichbarkeit}%
    \label{fig:1}%
\end{figure}

% Dichte-Erreichbarkeit

\newpage

\textbf{Dichte-Erreichbarkeit}\\
Die Erweiterung der \textit{direkten Dichte-Erreichbarkeit} ist die im folgenden definierte \textit{Dichte-Erreichbarkeit}. Dabei sind $p$ und $q$ Punkte in der Datenmenge und es gibt eine Kette von \textit{direkt dichte-erreichbaren} Punkten, welche am Anfang \texit{direkt dichte-erreichbar} zu $p$ ist und $p$ \textit{direkt dichte-erreichbar} zum Ende der Kette ist. Formal definiert wird die \textit{Dichte-Erreichbarkeit} folgendermaßen:\\
Ein Punkt $ p \in D $ ist \textit{dichte-erreichbar} von $q$, falls
\begin{itemize}
    \item $\exists\enspace w_1,\dots,w_k \in D$
        \SubItem{$w_1$ ist \textit{direkt dichte-erreichbar} von $p$}
        \SubItem{$w_{i+1}$ ist \textit{direkt dichte-erreichbar} von $w_i$}
        \SubItem{$q$ ist \textit{direkt dichte-erreichbar} von $w_k$}
\end{itemize}
In Abbildung \ref{fig:2}a) sind die Punkte A und B \textit{dichte-verbunden}, da es eine Kette von \texit{direkt dichte-erreichbaren} Punkten gibt, welche die beiden Punkte verbinden.\\

% Dichte-Verbundenheit

\textbf{Dichte-Verbundenheit}\\

Ein Punkt $ p \in D $ ist \textit{dichte-verbunden} zu $p$, falls
\begin{itemize}
    \item $\exists\enspace w \in D$ sodass...
        \SubItem{ $p$ ist \textit{dichte-erreichbar} von $o$}
        \SubItem{ $q$ ist \textit{dichte-erreichbar} von $o$}
\end{itemize}

% Abbildung 2

\begin{figure}[hb]
    \centering
    \subfloat[]{{\includesvg[width=0.45\textwidth]{figures/figure3.svg}}}%
    \qquad
    \subfloat[]{{\includesvg[width=0.45\textwidth]{figures/figure4.svg}}}
    \caption{Dichte-Erreichbarkeit und Dichte-Verbundenheit}%
    \label{fig:2}%
\end{figure}

% Cluster
    
\textbf{Cluster}\\
Ein Cluster $ C \subseteq D $ ist definiert, sodass
\begin{itemize}
    \item $\forall\enspace p,q \in D$ gilt, falls
        \SubItem{$p \in C$}
        \SubItem{$q$ ist \textit{dichte-erreichbar} von $p$}
        
    $\Rightarrow q \in C$
    
    \item $\forall\enspace p,q \in C$ gilt $p$ ist \textit{dichte-erreichbar} von $q$
\end{itemize}

% Noise
    
\textbf{Noise}\\
Der Noise der Datenmenge $D$ mit den Clustern $C_1,\dots,C_k$ ist definiert als
\begin{itemize}
    \item $ \{ p \in D \enspace | \enspace p \notin C_1,\dots,C_k \} $
\end{itemize}

%
% Implementierung
%

\subsection{Implementierung}
\label{sec:impl}

\newpage

\begin{lstlisting}[caption={DBSCAN},label={lst:dbscan}]
void DBSCAN(data, eps, minPts) {
    // Jeder Punkt in data hat clusterID == NONE (kein Cluster)
    clusterID = 1;
    for point in data {
        if point.clusterID == NONE {
            if ExpandCluster(data, point, clusterID, eps, minPts) {
                // Ein neues Cluster wurde gefunden
                clusterID++;
            }
        }
    }
}
\end{lstlisting}

\begin{lstlisting}[caption={ExpandCluster},label={lst:expcluster}]
bool ExpandCluster(data, point, clusterID, eps, minPts) {
    seeds = data.reqionQuery(point, eps);
    if seeds.size < minPts {
        point.clusterID = NOISE;
        return false;
    } else {
        seeds.clusterID = clusterID;
        seeds.remove(point);
        while seeds.size > 0 {
            curPoint = seeds.getFirst();
            curRegion = data.regionQuery(curPoint, eps);
            if curRegion.size >= minPts {
                for regPoint in curRegion {
                    if regPoint.clusterID == NONE {
                        regPoint.clusterID = clusterID;
                        seeds.append(regPoint);
                    } else if regPoint.clusterID == NOISE {
                        regPoint.clusterID = clusterID;
                    }
                }
            }
            seeds.remove(curPoint);
        }
        return true;
    }
}
\end{lstlisting}

%
% Performance
%

\section{Performance}
\label{sec:perf}

Kriterien:\\
- Clustering\\
- Laufzeit\\
- Speicherplatz\\
- Aktualit\\

\cite{REV} \cite{REVREV} \cite{PPF}

%
% Fazit
%

\section{Fazit}
\label{sec:fazit}

Beurteilung von DBSCAN anhand des Performance Vergleichs.

\bibliography{references}

\end{document}
